# C++: Twitter Sentiment Analysis with Logistic Regression
I implemented a Logistic Regression model to classify tweets as either negative or positive by implementing my own Gradient Descent algorithm from scratch. I spent approximately a month working on the C++ implementation of this project, then completed my Python implementation of the project over a 2-day period. I'm excited to share my C++ code here, because I learned so much implementing Logistic Regression from scratch using Gradient Descent and custom data structure classes. Please enjoy! ðŸ˜ƒ
## Implementation Steps ðŸ“š
### Creating my Own String and Vector Classes
Instead, of using standard template libraries for strings and vectors, this project challenged me to further understand data structures by implementing my own DSString and DSVector classes from scratch. I developed a much stronger understanding of Object-Oriented programming, assignment operators, iterators, and more by creating my own template classes.
### Reading in the Training File
While developing my own string class, I came across the obstable of reading in the file without standard template library strings. I used a stringstream to perform this task, then immediately converted the stringstream to a DSString type. From there, I used a split() function that I incorporated into my DSString class to parse through the gigantic DSString and create a 2D-DSVector reminiscent of a Pandas DataFrame in Python to store my data. 
### Cleaning the Data
Within my DSString class, I included stemming(), to_lowercase(), remove_punctuation(), and remove_repeat_chars() functions to handle the cleaning of Tweet data easily. I clean the data using these methods to ensure that my feature extraction model would fit as accurately as it possibly could to the data. For example, words like "jumps", "JUMP", "jumppp!", and "jumped" would all be equated to "jump" and be evaluated as such when put into perspective with all their other appearances in Tweets during the feature extraction process.
### Creating a "Dictionary"
I created a 1D DSVector to store Word objects, where each Word contains private variables denoting the word as a DSString and two integers denoting the number of positive and negative appearances in the training set respectively. I tried to create this "dictionary" as a C++ unordered_map, so I could achieve O(1) search instead of O(n) search, but I could not get past bugs that required hashing elements of my key to overcome.
### Feature Extraction
I developed a feature extraction method on my own to numerically represent my data. I used the positive and negative appearances stored in my dictionary for each word in the tweet to score the overall Tweet. I then created a (20,000 x 3) DSVector containing 1's in the first column, positive training set appearances for all Tweet words in the second column for each Tweet, and negative training set appearances in the third column. The runtime is flawed because it's Big-Oh notation is O(3*n^2) => O(n^2) [could have been O(n^2) using an unordered_map]. Regardless, my feature extraction method evaluates each Tweet based off of each word's data with respect to its positive and negative frequency within the entire training set, which is why it's so powerful.
### Training my Logistic Regression Model with Gradient Descent
I coded a Gradient Descent function from scratch that returns the estimated population parameters for our Logistic Regression model. Computing the gradient and applying it to this process was difficult, but to qualitatively describe the process, I basically coded a function that trends towards the smallest deviation in reality compared to our estimated Logistic Regression model as possible as we iterate through it. By coding this algorithm that climbs down a mathematical function denoting this deviation and finding its global minimum, we are able to return parameters that perform well at predicting positive and negative Tweet sentiments.  
### Applying my Logistic Regression Model to the Training Data
I used the same methods of reading, cleaning, and feature extracting for my testing data. I then used the logistic regression parameters which I retrieved from my Gradient Descent function to predict testing Tweet sentiments. If the Tweet prediction was above 0.5 after inputing the positive and negative Training set appearances for all the words within the Tweet, I predicted a positive sentiment, and I predicted a negative sentiment when that wasn't the case. I used this method, because the decimal in the sigmoid function represents the probability of our assuming our model features are perfect (which they are not). 
### Results
Using my method of feature extraction, my Logistic Regression model predicted the testing data sentiments correctly 69.79% of the time!
## Take-Aways ðŸ’¡
While my approach had some flaws, and adjustments in both my feature extraction model and my data cleaning methods could have been made to improve the accuracy, I am very happy with my Classification model and how I implemented a machine learning model from scratch in C++. Keeping track of my variables was difficult, especially when I resorted to using several 2D-DSVectors, but I definitely strengthened my ability to visualize data and dissect 
After finishing this project in C++, that's where I decided to segue over to Python and observe my results over there. 
